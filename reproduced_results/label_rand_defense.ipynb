{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducing Label Randomization Defense Results (Cora dataset)\n",
    "\n",
    "To reproduce the results for the Label Randomization defense on the Cora dataset:\n",
    "\n",
    "1. Open `scripts/run_defense_label_rand.sh`\n",
    "2. Modify as needed:\n",
    "   - `datasets` (default=Cora)\n",
    "   - `experiment_name` (Name for the experiment, will be used as directory name for logs)\n",
    "   - `label_defense_budgets` (Array of budget values to test for Label Randomization)\n",
    "   - Modify the parallel parameter (-j) to adjust the number of concurrent jobs based on your system's capabilities\n",
    "\n",
    "3. Make the script executable:\n",
    "   ```\n",
    "   chmod +x scripts/run_defense_label_rand.sh\n",
    "   ```\n",
    "\n",
    "4. Run the script:\n",
    "   ```\n",
    "   bash scripts/run_defense_label_rand.sh\n",
    "   ```\n",
    "\n",
    "Results will be logged locally and saved in the `logs/{experiment_name}` directory.\n",
    "\n",
    "### Note on Label Randomization Defense:\n",
    "Label Randomization defense randomly changes a portion of the training labels. This affects all attacks except the feature attack, as it modifies the training process and the resulting model. The `label_defense_budget` parameter determines the proportion of labels that are randomized.\n",
    "\n",
    "### Parsing the results\n",
    "\n",
    "After running the script, the logs of the experiments are stored in the directory `logs/Defense Label Randomization results Cora`.\n",
    "The code below will parse these results to compute the average accuracy for the Gradient, Inter-Reps, Output Server, and Label attacks across multiple seeds and label defense budget values.\n",
    "\n",
    "The attack methods are renamed in the results to match the names used in the paper:\n",
    "- 'gradients' is used for the Gradient attack\n",
    "- 'forward_values' is renamed to 'Inter-Reps'\n",
    "- 'output_server' represents the Prediction Output attack\n",
    "- 'labels' represents the Label attack\n",
    "\n",
    "The resulting table will show how the accuracy of these four attacks changes with different label defense budget values for the Label Randomization defense. Note that the feature attack is not included as it remains unaffected by this defense mechanism.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "\n",
    "def safe_float(x):\n",
    "    try:\n",
    "        return float(x)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "def parse_results(experiment_dir):\n",
    "    results = []\n",
    "    for subdir in os.listdir(experiment_dir):\n",
    "        csv_path = os.path.join(experiment_dir, subdir, 'attack_results.csv')\n",
    "        config_path = os.path.join(experiment_dir, subdir, 'config.json')\n",
    "        metrics_path = os.path.join(experiment_dir, subdir, 'metrics.csv')\n",
    "        \n",
    "        if os.path.exists(csv_path) and os.path.exists(config_path) and os.path.exists(metrics_path):\n",
    "            with open(config_path, 'r') as f:\n",
    "                config = json.load(f)\n",
    "                label_defense_budget = config.get('label_defense_budget', None)\n",
    "            \n",
    "            if label_defense_budget is None:\n",
    "                continue\n",
    "            \n",
    "            df = pd.read_csv(csv_path)\n",
    "            accuracy_df = df[df['metric'].str.startswith('Accuracy-')].copy()\n",
    "            accuracy_df['value'] = accuracy_df['value'].apply(safe_float)\n",
    "            \n",
    "            metrics_df = pd.read_csv(metrics_path)\n",
    "            last_epoch = metrics_df['epoch'].max()\n",
    "            test_accuracy_row = metrics_df[(metrics_df['epoch'] == last_epoch) & (metrics_df['metric'] == 'accuracy_test')]\n",
    "            \n",
    "            if test_accuracy_row.empty:\n",
    "                print(f\"Warning: No test accuracy found for {subdir}\")\n",
    "                continue\n",
    "            \n",
    "            test_accuracy = test_accuracy_row['value'].iloc[0]\n",
    "            \n",
    "            run_result = {\n",
    "                'label_defense_budget': label_defense_budget,\n",
    "                'seed': subdir,\n",
    "                'test_accuracy': test_accuracy\n",
    "            }\n",
    "            \n",
    "            for attack in ['gradients', 'forward_values', 'output_server', 'labels']:\n",
    "                attack_results = accuracy_df[accuracy_df['metric'] == f'Accuracy-{attack}']\n",
    "                if not attack_results.empty:\n",
    "                    if attack == 'gradients':\n",
    "                        accuracy = attack_results['value'].iloc[0]  # First epoch for gradient attack\n",
    "                    else:\n",
    "                        accuracy = attack_results['value'].iloc[-1]  # Last epoch for other attacks\n",
    "                    attack_name = 'Inter-Reps' if attack == 'forward_values' else 'Gradients' if attack == 'gradients' else 'Prediction Output' if attack == 'output_server' else 'Label'\n",
    "                    run_result[f'{attack_name}_accuracy'] = accuracy\n",
    "            \n",
    "            results.append(run_result)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "experiment_dir = '../logs/Defense Label Randomization results Cora'\n",
    "results_df = parse_results(experiment_dir)\n",
    "\n",
    "if results_df.empty:\n",
    "    print(\"No valid results found. Please check the experiment directory and data.\")\n",
    "else:\n",
    "    # Group results by label_defense_budget and calculate mean and sem for all metrics\n",
    "    grouped_results = results_df.groupby('label_defense_budget').agg({\n",
    "        'Gradients_accuracy': ['mean', 'sem'],\n",
    "        'Inter-Reps_accuracy': ['mean', 'sem'],\n",
    "        'Prediction Output_accuracy': ['mean', 'sem'],\n",
    "        'Label_accuracy': ['mean', 'sem'],\n",
    "        'test_accuracy': ['mean', 'sem']\n",
    "    }).reset_index()\n",
    "\n",
    "    # Flatten column names\n",
    "    grouped_results.columns = ['_'.join(col).strip() for col in grouped_results.columns.values]\n",
    "\n",
    "    # Rename columns for clarity\n",
    "    grouped_results = grouped_results.rename(columns={\n",
    "        'label_defense_budget_': 'label_defense_budget',\n",
    "        'Gradients_accuracy_mean': 'Gradients_mean',\n",
    "        'Gradients_accuracy_sem': 'Gradients_sem',\n",
    "        'Inter-Reps_accuracy_mean': 'Inter-Reps_mean',\n",
    "        'Inter-Reps_accuracy_sem': 'Inter-Reps_sem',\n",
    "        'Prediction Output_accuracy_mean': 'Prediction Output_mean',\n",
    "        'Prediction Output_accuracy_sem': 'Prediction Output_sem',\n",
    "        'Label_accuracy_mean': 'Label_mean',\n",
    "        'Label_accuracy_sem': 'Label_sem',\n",
    "        'test_accuracy_mean': 'test_accuracy_mean',\n",
    "        'test_accuracy_sem': 'test_accuracy_sem'\n",
    "    })\n",
    "\n",
    "    # Sort by label_defense_budget in ascending order\n",
    "    grouped_results = grouped_results.sort_values('label_defense_budget', ascending=True)\n",
    "\n",
    "    # Format the table\n",
    "    table_data = []\n",
    "    for _, row in grouped_results.iterrows():\n",
    "        table_data.append([\n",
    "            f\"{row['label_defense_budget']:.2f}\",\n",
    "            f\"{row['Gradients_mean']*100:.2f} ± {row['Gradients_sem']*100:.2f}\",\n",
    "            f\"{row['Inter-Reps_mean']*100:.2f} ± {row['Inter-Reps_sem']*100:.2f}\",\n",
    "            f\"{row['Prediction Output_mean']*100:.2f} ± {row['Prediction Output_sem']*100:.2f}\",\n",
    "            f\"{row['Label_mean']*100:.2f} ± {row['Label_sem']*100:.2f}\",\n",
    "            f\"{row['test_accuracy_mean']*100:.2f} ± {row['test_accuracy_sem']*100:.2f}\"\n",
    "        ])\n",
    "\n",
    "    # Create the table\n",
    "    table = tabulate(table_data, headers=[\"Label Defense Budget\", \"Gradients\", \"Inter-Reps\", \"Prediction Output\", \"Label\", \"Test Accuracy\"], \n",
    "                     tablefmt=\"pipe\", floatfmt=\".2f\")\n",
    "\n",
    "    print(table)\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for attack in ['Gradients', 'Inter-Reps', 'Prediction Output', 'Label', 'test_accuracy']:\n",
    "        mean_col = f'{attack}_mean'\n",
    "        sem_col = f'{attack}_sem'\n",
    "        label = 'Test Accuracy' if attack == 'test_accuracy' else attack\n",
    "        plt.errorbar(grouped_results['label_defense_budget'], grouped_results[mean_col]*100, yerr=grouped_results[sem_col]*100, \n",
    "                     label=label, capsize=5, marker='o')\n",
    "\n",
    "    plt.xlabel('Label Defense Budget')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Label Randomization Defense Results (Cora)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
