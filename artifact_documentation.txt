\documentclass{article}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{longtable}

\title{Artifact Documentation:\\
Link Inference Attacks in Vertical Federated Graph Learning\\
\\
\\
\large Submission Number: 342}

\begin{document}

\maketitle

\section*{Acknowledgment}
We sincerely thank the artifact reviewers for choosing our paper for evaluation. Your time and effort in assessing our work are greatly appreciated.

\section{Introduction}
This document provides detailed instructions for reproducing the main results of the paper "Link Inference Attacks in Vertical Federated Graph Learning" (submission number 342). The artifacts are hosted on \href{https://github.com/username/link-inference-attack}{GitHub} and include all necessary code, datasets, and scripts to replicate our experiments. These artifacts specifically reproduce the following key results from our paper:

\begin{itemize}
\item Table 4: Accuracy of link inference attacks across datasets using GCN architecture
\item Figure 3: Impact of the number of training epochs on attack performance
\item Figure 4: Comparison of AUC between Gradient-based LIA, Intermediate Representations-based LIA, and Feature-based LIA across different feature ratios of the adversary
\item Results of the defense mechanisms introduced in Section 6, including LapGraph and Label Perturbation
\end{itemize}

This documentation is structured as follows:
\begin{itemize}

\item Section 2 outlines the repository structure and provides a comprehensive file directory
\item Section 3 covers requirements and setup instructions
\item Section 4 describes hardware requirements for running the experiments
\item Section 5 provides step-by-step guidelines to reproduce results, including:
  \begin{itemize}
  \item Detailed steps to run experiments
  \item A recommended workflow for result reproduction
  \item Instructions for reproducing specific results (e.g., Table 4)
  \end{itemize}
\item Section 6 provides execution time estimates for different datasets
\item Section 7 offers supplementary materials and alternative workflows
\end{itemize}

By following this documentation, you will be able to replicate our experimental setup, run the necessary scripts, and generate the key results and figures presented in our paper.

\section{Repository Structure and File Directory}
\begin{longtable}{p{0.45\textwidth}p{0.55\textwidth}}
\toprule
File/Directory Name & Description \\
\midrule
\texttt{setup\_lia\_vfgl.sh} & Shell script for setting up the environment and installing required libraries \\
\texttt{requirements.txt} & List of required Python libraries and their versions \\
\texttt{datasets/} & Directory containing datasets used in experiments, downloaded upon environment installation \\
\texttt{defense/} & Directory containing implemented defenses from our paper \\
\texttt{defense/lapgraph.py} & LapGraph defense introduced in Section 6.1 of the revised paper \\
\texttt{defense/} & \\
\texttt{label\_defense.py} & Label perturbation defense introduced in Section 6.2 of the revised paper \\
\texttt{utils/} & Directory containing utility functions such as argument parser and dataset loaders \\
\texttt{attack.py} & Implementation of the introduced attacks \\
\texttt{label\_based\_attack.py} & Implementation of the label-based attack \\
\texttt{main.py} & Main function that runs all experiments \\
\texttt{models.py} & Implementation of the ML models used in the paper \\
\texttt{download\_datasets.py} & Script to download datasets, included in the environment setup script. Can be used to download datasets without creating the environment \\
\texttt{scripts/} & Directory containing experiment scripts \\
\texttt{scripts/run\_figure3.sh} & Script to reproduce Figure 3 results \\
\texttt{scripts/run\_figure4.sh} & Script to reproduce Figure 4 results \\
\texttt{scripts/run\_table4.sh} & Script to reproduce Table 4 results \\
\texttt{scripts/} & \\
\texttt{run\_defense\_label\_rand.sh} & Script to reproduce Label Perturbation defense results \\
\texttt{scripts/} & \\
\texttt{run\_defense\_lapgraph.sh} & Script to reproduce LapGraph defense results \\
\texttt{log/} & Directory where experiment logs are stored \\
\texttt{reproduced\_results/} & Directory containing Jupyter notebooks and Python scripts for parsing logs and producing final results \\
\texttt{reproduced\_results/} & \\
\texttt{figure3.py} & Python script to generate Figure 3 from log data \\
\texttt{reproduced\_results/} & \\
\texttt{figure4.py} & Python script to generate Figure 4 from log data \\
\texttt{reproduced\_results/} & \\
\texttt{table4.py} & Python script to generate Table 4 from log data \\
\texttt{reproduced\_results/} & \\
\texttt{label\_rand\_defense.py} & Python script to generate Label Randomization defense results \\
\texttt{reproduced\_results/} & \\
\texttt{lapgraph\_defense.py} & Python script to generate LapGraph defense results \\
\bottomrule
\end{longtable}

\section{Requirements and Setup}
To set up the environment and install all required libraries:

\begin{enumerate}
\item Run the \texttt{setup\_lia\_vfgl.sh} script:
\begin{verbatim}
./setup_lia_vfgl.sh
\end{verbatim}
This script will install the necessary environment with all required libraries and download all datasets used in the paper.

\item The main requirements are listed in \texttt{requirements.txt}, including:
\begin{itemize}
\item PyTorch 1.13.0
\item PyTorch Geometric 2.2.0
\item NumPy 1.23.5
\item Pandas 1.5.2
\item Matplotlib 3.6.2
\item Wandb 0.13.6
\item Scikit-learn 1.2.0
\end{itemize}

\item Activate the environment before running any experiments:
\begin{verbatim}
source lia_vfgl_env/bin/activate
\end{verbatim}
The name of the environment (lia\_vfgl\_env) should be apparent in your command prompt after activation.
\end{enumerate}

\section{Hardware Requirements}
Our results were primarily produced using the following setup:
\begin{itemize}
\item 4x NVIDIA GeForce RTX 3090 GPUs (24GB VRAM each)
\item AMD EPYC 7272 12-Core Processor
\item 125GB RAM
\item Ubuntu 20.04.6 LTS with Linux kernel 5.4.0-177-generic
\end{itemize}

We have also successfully tested the Table 4 experiment for the Cora dataset on the Artifacts submission platform's Compute VM, which features a 16-core CPU, 64GB memory, and runs Ubuntu 22.04.

Note: The Cora dataset experiments do not require a GPU and can be run in a reasonable time on the smaller setup.

\section{Reproducing Results}
To reproduce the results of the paper, follow these steps:

\subsection{Detailed Steps to Run Experiments}
\begin{enumerate}
\item Navigate to the \texttt{scripts} directory.
\item Run the appropriate script for each experiment. Each script is associated with a specific experiment and will log results in the \texttt{log} directory.
\item After the script execution is complete, use one of the following methods to parse logs and produce final tables and graphs:
  \begin{itemize}
  \item Use the corresponding Jupyter notebook in the \texttt{reproduced-results} directory.
  \item Run the associated Python script in the same directory.
  \end{itemize}
\end{enumerate}

\subsection{Recommended Workflow}
\begin{enumerate}
\item Go to the \texttt{reproduced-results} directory.
\item Read the \texttt{.ipynb} file associated with the experiment you want to reproduce.
\item Follow the detailed instructions in the notebook on how to run the corresponding script.
\item After the script has finished executing, choose one of these options:
  \begin{itemize}
  \item Run the cells in the Jupyter notebook to generate the final results.
  \item Execute the corresponding \texttt{.py} script to parse logs and save tables and figures.
  \end{itemize}
\end{enumerate}

\subsection{Reproducing Table 4 and Other Results}
To reproduce Table 4 and other results from the paper:
\begin{enumerate}
\item Locate the corresponding script at \texttt{scripts/run\_table4.sh}.
\item Run the script following the instructions in the associated Jupyter notebook \texttt{reproduced\_results/table4.ipynb}.
\item Once the script has finished executing, open the Jupyter notebook and run its cells to generate the final results.
\end{enumerate}

\section{Execution Time Estimation}
Estimated time for reproduction varies by dataset and experiment. These estimates are for 5 parallel runs on our primary hardware setup:

\begin{itemize}
\item Cora and Citeseer: 3 minutes
\item Amazon Photo: 22 minutes
\item Amazon Computer: 77 minutes
\item Twitch FR: 30 minutes
\item Twitch DE: 55 minutes
\item Twitch EN: 30 minutes
\end{itemize}

Note that each experiment requires multiple runs based on the number of data points needed and the number of seeds. You can reduce the total execution time by lowering the number of seeds in the corresponding script files.

\section{Supplementary Materials and Alternative Workflows}
For detailed instructions on running each experiment, please refer to the corresponding Jupyter notebooks in the \texttt{reproduced\_results} directory.

If you prefer not to work with notebooks, each \texttt{.ipynb} file in the \texttt{reproduce\_results} directory has a corresponding \texttt{.py} script. These scripts parse the logs and save the tables and figures directly in the directory, providing an alternative to using the notebooks.

\end{document}
